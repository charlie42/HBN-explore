{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4128d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_FROM_FILE = 0\n",
    "IMPORTANCES_FROM_FILE = 0\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, recall_score, precision_score, balanced_accuracy_score, f1_score, roc_auc_score, fbeta_score, make_scorer, roc_curve, precision_recall_curve, accuracy_score\n",
    "from sklearn.metrics import PrecisionRecallDisplay, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "    \n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "\n",
    "data_intermediate_dir = \"data/intermediate/\"\n",
    "item_lvl_w_imp = pd.read_csv(data_intermediate_dir + \"item_lvl_wo_impairment.csv\")\n",
    "\n",
    "# Prepare input and ouptut column names\n",
    "\n",
    "output_cols = [x for x in item_lvl_w_imp.columns if x.startswith(\"WHODAS\") or x.startswith(\"CIS\")] # All impairment columns\n",
    "\n",
    "input_cols = [x for x in item_lvl_w_imp.columns if \n",
    "                       not x.startswith(\"Diag: \") \n",
    "                       and not x.startswith(\"WIAT\")\n",
    "                       and not x.startswith(\"WISC\")\n",
    "                       and not x.startswith(\"WHODAS\")\n",
    "                       and not x.startswith(\"CIS\")] # Input columns are all columns except Diagnosis, WIAT, and WISC, impairment columns\n",
    "\n",
    "# Separate test set for all impairment scores\n",
    "\n",
    "# Shuffle the dataset \n",
    "shuffle_df = item_lvl_w_imp.sample(frac=1, random_state=42)\n",
    "\n",
    "# Define a size for the train set \n",
    "train_size = int(0.7 * len(item_lvl_w_imp))\n",
    "\n",
    "# Split the dataset \n",
    "train_set = shuffle_df[:train_size]\n",
    "test_set = shuffle_df[train_size:] # Don't touch this until the end\n",
    "\n",
    "# Train_train and Validation set\n",
    "# Define a size for your train_train set \n",
    "train_train_size = int(0.7 * len(train_set))\n",
    "\n",
    "# Split your dataset \n",
    "train_train_set = train_set[:train_train_size]\n",
    "val_set = train_set[train_train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7be13ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_models_and_param_grids():\n",
    "    \n",
    "    # Define base models\n",
    "    dt = DecisionTreeRegressor()\n",
    "    rf = RandomForestRegressor()\n",
    "    svr = svm.SVR()\n",
    "    en = ElasticNet(tol=1e-3)\n",
    "    \n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Make pipelines\n",
    "    dt_pipe = make_pipeline(imputer, scaler, dt)\n",
    "    rf_pipe = make_pipeline(imputer, scaler, rf)\n",
    "    svr_pipe = make_pipeline(imputer, scaler, svr)\n",
    "    en_pipe = make_pipeline(imputer, scaler, en)\n",
    "    \n",
    "    # Define parameter grids to search for each pipe\n",
    "    from scipy.stats import loguniform, uniform\n",
    "    dt_param_grid = {\n",
    "        \"decisiontreeregressor__splitter\": [\"best\",\"random\"],\n",
    "        \"decisiontreeregressor__min_samples_split\": np.random.randint(2, 20, 30),\n",
    "        \"decisiontreeregressor__max_depth\": np.random.randint(1, 30, 30),\n",
    "        \"decisiontreeregressor__min_samples_leaf\": np.random.randint(1, 20, 30),\n",
    "        \"decisiontreeregressor__max_leaf_nodes\": np.random.randint(2, 50, 30)\n",
    "    }\n",
    "    rf_param_grid = {\n",
    "        'randomforestregressor__max_depth' : np.random.randint(5, 150, 30),\n",
    "        'randomforestregressor__min_samples_split': np.random.randint(2, 50, 30),\n",
    "        'randomforestregressor__n_estimators': np.random.randint(50, 400, 10),\n",
    "        'randomforestregressor__min_samples_leaf': np.random.randint(1, 20, 30),\n",
    "        'randomforestregressor__max_features': ['auto', 'sqrt', 'log2', 0.25, 0.5, 0.75, 1.0]\n",
    "    }\n",
    "    svr_param_grid = {\n",
    "        'svr__C': loguniform(1e-03, 1e+02),\n",
    "        'svr__gamma': loguniform(1e-03, 1e+02),\n",
    "        'svr__degree': uniform(2, 5),\n",
    "        'svr__epsilon': loguniform(1e-03,1),\n",
    "        'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }\n",
    "    en_param_grid = {\n",
    "        'elasticnet__alpha': loguniform(1e-5, 100),\n",
    "        'elasticnet__l1_ratio': uniform(0, 1)\n",
    "    }\n",
    "    \n",
    "    base_models_and_param_grids = [\n",
    "        (dt_pipe, dt_param_grid),\n",
    "        (rf_pipe, rf_param_grid),\n",
    "        (svr_pipe, svr_param_grid),\n",
    "        (en_pipe, en_param_grid),\n",
    "    ]\n",
    "    \n",
    "    return base_models_and_param_grids\n",
    "\n",
    "def get_best_classifier(base_model, grid, output_col):\n",
    "    rs = RandomizedSearchCV(estimator=base_model, param_distributions=grid, cv=3, scoring=\"r2\", n_iter=100, n_jobs = -1)\n",
    "    \n",
    "    rs.fit(train_set[input_cols], train_set[output_col]) # On train_set, not train_train_set because do cross-validation\n",
    "    \n",
    "    best_estimator = rs.best_estimator_\n",
    "    best_score = rs.best_score_\n",
    "    \n",
    "    # If chosen model is SVM add a predict_proba parameter (not needed for grid search, and slows it down significantly)\n",
    "    if 'svc' in best_estimator.named_steps.keys():\n",
    "        best_estimator.set_params(svc__probability=True)\n",
    "\n",
    "    return (best_estimator, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a4fc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_estimator_for_output(output_col):\n",
    "    best_score = 0\n",
    "    best_classifier = None\n",
    "    base_models_and_param_grids = get_base_models_and_param_grids()\n",
    "    for (base_model, grid) in base_models_and_param_grids:\n",
    "        best_classifier_for_model, best_score_for_model = get_best_classifier(base_model, grid, output_col)\n",
    "        if best_score_for_model > best_score:\n",
    "            best_classifier = best_classifier_for_model\n",
    "            best_score = best_score_for_model\n",
    "    return best_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f38b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELS_FROM_FILE == 0:\n",
    "    best_estimators = {}\n",
    "    for output in output_cols:\n",
    "        best_estimator_for_output = find_best_estimator_for_output(output)\n",
    "        best_estimators[output] = best_estimator_for_output\n",
    "        print(\"Best estimator for \", output, \": \", best_estimators[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "if MODELS_FROM_FILE == 0:\n",
    "    # Save best estimators - best parameters, and the estimator objects themselves\n",
    "    best_estimators_df = pd.DataFrame(best_estimators).T.reset_index()\n",
    "    best_estimators_df.columns = [\"Output\", \"Best Estimator\"]\n",
    "    display(best_estimators_df)\n",
    "    best_estimators_df.to_csv(\"data/output/best-estimators-imp-item-lvl.csv\")\n",
    "\n",
    "    dump(best_estimators, 'best-estimators-imp-item-lvl.joblib', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELS_FROM_FILE == 1:\n",
    "    best_estimators = load('best-estimators-imp-item-lvl.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9fbb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['r2']   \n",
    "def get_metrics(estimator, output_col, input_cols, validation_or_test = \"validation\", print_output_flag = 0):\n",
    "    input_cols = input_cols\n",
    "    \n",
    "    if validation_or_test == \"test\":\n",
    "        x = test_set[input_cols]\n",
    "        y = test_set[output_col]\n",
    "    else: \n",
    "        x = val_set[input_cols]\n",
    "        y = val_set[output_col]\n",
    "        \n",
    "    y_pred = estimator.predict(x)\n",
    "    \n",
    "    metrics = []\n",
    "    r2 = r2(y, y_pred)\n",
    "    metrics.append(r2)\n",
    "    \n",
    "    if print_output_flag:\n",
    "        print(output_col)\n",
    "        print(\"r2: \", r2)\n",
    "        \n",
    "        plt.scatter(y, y_pred)\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cross-validation performance of the final model on validation set for all models\n",
    "def plot_test_vs_train_scores(cv_results):\n",
    "    scores = pd.DataFrame()\n",
    "    scores[[\"Tran R2\", \"Test R2\"]] = cv_results[[\"train_score\", \"test_score\"]]\n",
    "    scores.plot.hist(bins=50, edgecolor=\"black\")\n",
    "    plt.show()\n",
    "\n",
    "results_val_set = []\n",
    "for output in output_cols:\n",
    "    estimator = best_estimators[output]\n",
    "    cv_results = cross_validate(train_set[input_cols], train_set[output], return_train_score = True)\n",
    "    metrics = [mean(cv_results[\"test_score\"]), std(cv_results[\"test_score\"])]\n",
    "    results_val_set.append([\n",
    "        output, \n",
    "        *metrics])\n",
    "    \n",
    "    plot_test_vs_train_scores(cv_results)\n",
    "restults_val_set_df = pd.DataFrame(results_val_set, columns=[\"Output\"] + [\"Mean R2\", \"Std R2\"])\n",
    "restults_val_set_df.sort_values(by=\"r2\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance of the final model on test set for all models\n",
    "\n",
    "results_test_set = []\n",
    "for output in output_cols:\n",
    "    estimator = best_estimators[output]\n",
    "    metrics = get_metrics(estimator, threshold, output, input_cols, 'test', 1)\n",
    "    results_test_set.append([\n",
    "        output, \n",
    "        *metrics])\n",
    "restults_test_set_df = pd.DataFrame(results_test_set, columns=[\"Output\"] + metric_names)\n",
    "restults_test_set_df.sort_values(by=\"r2\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a20e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORTANCES_FROM_FILE == 1:\n",
    "    forward_feature_objects = {}\n",
    "    backward_feature_objects = {}\n",
    "    try:\n",
    "        forward_feature_objects = load('forward-sfs-imp-item-lvl.joblib')\n",
    "    except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find elbow of the curve (draw a line from the first to the last point of the curve and then find the data point that is farthest away from that line) https://stackoverflow.com/questions/2018178/finding-the-best-trade-off-point-on-a-curve \n",
    "def find_elbow(curve):\n",
    "    from numpy import matlib \n",
    "    n_points = len(curve)\n",
    "    all_coord = np.vstack((range(n_points), curve)).T\n",
    "    first_point = all_coord[0]\n",
    "    line_vec = all_coord[-1] - all_coord[0]\n",
    "    line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n",
    "    vec_from_first = all_coord - first_point\n",
    "    scalar_product = np.sum(vec_from_first * np.matlib.repmat(line_vec_norm, n_points, 1), axis=1)\n",
    "    vec_from_first_parallel = np.outer(scalar_product, line_vec_norm)\n",
    "    vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "    dist_to_line = np.sqrt(np.sum(vec_to_line ** 2, axis=1))\n",
    "    idx_of_best_point = np.argmax(dist_to_line)\n",
    "    return idx_of_best_point + 1\n",
    "\n",
    "def analyze_importances(importances):\n",
    "    importances_list = []\n",
    "    for key in importances:\n",
    "        importances_list.append(importances[key]['avg_score'])\n",
    "    importances_df = pd.DataFrame(importances_list, index=importances.keys(), columns=[\"r2\"])\n",
    "    display(importances_df)\n",
    "\n",
    "    max_score = max(importances_list)\n",
    "    n_cols_max_score = importances_df[importances_df[\"r2\"] == max_score].index[0]\n",
    "    print(\"Max score: \", max_score, \" at \", n_cols_max_score, \" features\")\n",
    "    features_up_to_max_score = importances[n_cols_max_score][\"feature_names\"]\n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(importances_df)\n",
    "    plt.xticks(np.arange(1,100, 3))\n",
    "    plt.vlines(np.arange(1,100, 3), ymin=min(importances_list), ymax=max(importances_list), colors='purple', ls=':', lw=1)\n",
    "    plt.show()\n",
    "    \n",
    "    optimal_nb_features = find_elbow(importances_list)\n",
    "    print(\"Optimal number of features: \", optimal_nb_features)\n",
    "    \n",
    "    features_up_to_optimal = importances[optimal_nb_features][\"feature_names\"]\n",
    "    display(features_up_to_optimal)\n",
    "    \n",
    "    return (features_up_to_optimal, features_up_to_max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "if IMPORTANCES_FROM_FILE == 0:\n",
    "    forward_feature_subsets = {}\n",
    "    forward_feature_objects = {}\n",
    "    for output in output_columns\n",
    "        print(output)\n",
    "        estimator = best_estimators[output]\n",
    "\n",
    "        sfs = SequentialFeatureSelector(estimator, \n",
    "              k_features=100,\n",
    "              forward=True, \n",
    "              scoring='r2',\n",
    "              cv=3,\n",
    "              n_jobs=-1)\n",
    "\n",
    "        sfs = sfs.fit(train_set[input_cols], train_set[output])\n",
    "\n",
    "        forward_feature_subsets[output] = sfs.subsets_\n",
    "        forward_feature_objects[output] = sfs\n",
    "        \n",
    "        analyze_importances(forward_feature_objects[output].subsets_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf08d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save importances to file\n",
    "if IMPORTANCES_FROM_FILE == 0:\n",
    "    dump(forward_feature_objects, 'forward-sfs-imp-item-lvl.joblib', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sfs results\n",
    "for output in output_cols:\n",
    "    print(output)\n",
    "    \n",
    "    features_up_to_optimal, features_up_to_max_score = analyze_importances(forward_feature_objects_all[output].subsets_)\n",
    "    features_up_to_optimal = list(features_up_to_optimal)\n",
    "    features_up_to_max_score = list(features_up_to_max_score)\n",
    "    \n",
    "    # Test both subsets on test set\n",
    "    estimator = best_estimators[output]\n",
    "    \n",
    "    # All features\n",
    "    # Create new pipeline with the same params (need to re-train the imputer on less features)\n",
    "    new_estimator = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='median'), StandardScaler(), estimator[2])\n",
    "    new_estimator.fit(train_set[input_cols], train_set[output])\n",
    "    print(\"Score using all features: \")\n",
    "    metrics = get_metrics(new_estimator, output, input_cols, 'test', 1)\n",
    "    print(\"R2: \", metrics[metric_names.index(\"r2\")])\n",
    "    \n",
    "    # Create new pipeline with the same params (need to re-train the imputer on less features)\n",
    "    new_estimator = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='median'), StandardScaler(), estimator[2])\n",
    "    new_estimator.fit(train_set[features_up_to_optimal], train_set[output])\n",
    "    print(\"Performance on test set using optimal number of columns: \")\n",
    "    metrics = get_metrics(new_estimaotr, output, features_up_to_optimal, 'test', 1)\n",
    "    print(\"R2: \", metrics[metric_names.index(\"r2\")])\n",
    "\n",
    "    # Create new pipeline with the same params (need to re-train the imputer on less features)\n",
    "    new_estimator = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='median'), StandardScaler(), estimator[2])\n",
    "    new_estimator.fit(train_set[features_up_to_max_score], train_set[output])\n",
    "    print(\"Performance on test set using number of columns with maximum score\")\n",
    "    metrics = get_metrics(new_estimator, output, features_up_to_max_score, 'test', 1)\n",
    "    print(\"Recall: \", metrics[metric_names.index(\"Recall\")])\n",
    "    print(\"R2: \", metrics[metric_names.index(\"r2\")])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
